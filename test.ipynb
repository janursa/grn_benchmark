{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_labels(Y, n_components):\n",
    "    if n_components == Y.shape[1]:\n",
    "        return None, None, Y\n",
    "    label_reducer = TruncatedSVD(n_components=n_components, n_iter=10)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    Y_scaled = scaler.fit_transform(Y)\n",
    "    Y_reduced = label_reducer.fit_transform(Y_scaled)\n",
    "\n",
    "    return label_reducer, scaler, Y_reduced\n",
    "\n",
    "\n",
    "def train_transformer_k_means_learning(X, Y, n_components, num_epochs, batch_size,\n",
    "                                       d_model=128, early_stopping=5000, device='cuda', seed=18):\n",
    "    label_reducer, scaler, Y_reduced = reduce_labels(Y, n_components)\n",
    "    Y_reduced = Y_reduced.to_numpy()\n",
    "    Y = Y.to_numpy()\n",
    "    num_clusters = 2\n",
    "    validation_percentage = 0.1\n",
    "\n",
    "    # Create a K-Means clustering model\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=100)\n",
    "\n",
    "    # Fit the model to your regression targets (Y)\n",
    "    clusters = kmeans.fit_predict(Y)\n",
    "\n",
    "    # Initialize lists to store the training and validation data\n",
    "    X_train, Y_train = [], []\n",
    "    X_val, Y_val = [], []\n",
    "\n",
    "    # Iterate through each cluster\n",
    "    for cluster_id in range(num_clusters):\n",
    "        # Find the indices of data points in the current cluster\n",
    "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "        print(len(cluster_indices))\n",
    "        if len(cluster_indices) >= 20:\n",
    "            # Split the data points in the cluster into training and validation\n",
    "            train_indices, val_indices = train_test_split(cluster_indices, test_size=validation_percentage,\n",
    "                                                          random_state=seed)\n",
    "\n",
    "            # Append the corresponding data points to the training and validation sets\n",
    "            X_train.extend(X[train_indices])\n",
    "            Y_train.extend(Y_reduced[train_indices])  # Y_reduced for train Y for validation\n",
    "            X_val.extend(X[val_indices])\n",
    "            Y_val.extend(Y[val_indices])\n",
    "        else:\n",
    "            X_train.extend(X[cluster_indices])\n",
    "            Y_train.extend(Y_reduced[cluster_indices])  # Y_reduced for train Y for validation\n",
    "    # Convert the lists to numpy arrays if needed\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "    X_val, Y_val = np.array(X_val), np.array(Y_val)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
