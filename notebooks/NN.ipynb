{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiomics GRN inference evaluation\n",
    "## VAE-SEM model\n",
    "### by Jalil Nourisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:05:01.634640Z",
     "iopub.status.busy": "2024-05-28T09:05:01.634378Z",
     "iopub.status.idle": "2024-05-28T09:05:08.165929Z",
     "shell.execute_reply": "2024-05-28T09:05:08.165265Z",
     "shell.execute_reply.started": "2024-05-28T09:05:01.634613Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anndata\n",
      "  Using cached anndata-0.10.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting category_encoders\n",
      "  Using cached category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
      "  Using cached array_api_compat-1.7-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: exceptiongroup in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from anndata) (1.2.0)\n",
      "Collecting h5py>=3.1 (from anndata)\n",
      "  Using cached h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting natsort (from anndata)\n",
      "  Using cached natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from anndata) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from anndata) (23.2)\n",
      "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from anndata) (1.5.3)\n",
      "Requirement already satisfied: scipy>1.8 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from anndata) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from category_encoders) (1.4.0)\n",
      "Collecting statsmodels>=0.9.0 (from category_encoders)\n",
      "  Using cached statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting patsy>=0.5.1 (from category_encoders)\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2023.3.post1)\n",
      "Requirement already satisfied: six in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.2.0)\n",
      "Using cached anndata-0.10.7-py3-none-any.whl (122 kB)\n",
      "Using cached category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "Using cached array_api_compat-1.7-py3-none-any.whl (37 kB)\n",
      "Using cached h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "Using cached natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: array-api-compat, patsy, natsort, h5py, statsmodels, anndata, category_encoders\n",
      "Successfully installed anndata-0.10.7 array-api-compat-1.7 category_encoders-2.6.3 h5py-3.11.0 natsort-8.4.0 patsy-0.5.6 statsmodels-0.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install anndata category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:05:08.167970Z",
     "iopub.status.busy": "2024-05-28T09:05:08.167570Z",
     "iopub.status.idle": "2024-05-28T09:05:15.025220Z",
     "shell.execute_reply": "2024-05-28T09:05:15.024466Z",
     "shell.execute_reply.started": "2024-05-28T09:05:08.167941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "-----Seed Set!-----\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error \n",
    "import os\n",
    "import random \n",
    "import category_encoders \n",
    "\n",
    "\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "SEED = 0xCAFE\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('using device: cuda')\n",
    "else:\n",
    "    print('using device: cpu')\n",
    "    USE_GPU = False\n",
    "\n",
    "def seed_everything():\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print('-----Seed Set!-----')\n",
    "seed_everything()\n",
    "\n",
    "def sign_grn(grn):\n",
    "    grn_sign = grn.copy()\n",
    "    weights = grn_sign.weight\n",
    "    weights = [1 if weight>0 else -1 for weight in weights]\n",
    "    grn_sign.weight = weights\n",
    "    return grn_sign\n",
    "def shuffle_grn(grn):\n",
    "    grn_s = grn.copy()\n",
    "    grn_s['source'] = grn_s['source'].sample(frac=1).reset_index(drop=True)\n",
    "    grn_s['target'] = grn_s['target'].sample(frac=1).reset_index(drop=True)\n",
    "    dup_flags = grn_s[['source','target']].duplicated()\n",
    "    grn_s = grn_s[~dup_flags].reset_index(drop=True)\n",
    "    if grn_s.duplicated().sum()>0:\n",
    "        raise ValueError('')\n",
    "    return grn_s\n",
    "\n",
    "def mrrmse(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    return float(np.mean(np.sqrt(np.mean(np.square(x - y), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:05:15.026735Z",
     "iopub.status.busy": "2024-05-28T09:05:15.026227Z",
     "iopub.status.idle": "2024-05-28T09:05:19.276234Z",
     "shell.execute_reply": "2024-05-28T09:05:19.275532Z",
     "shell.execute_reply.started": "2024-05-28T09:05:15.026710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    adata = ad.read_h5ad('../output/preprocess/bulk_adata_f.h5ad')\n",
    "    hvgs = np.loadtxt('../output/hvgs.txt', dtype=str)\n",
    "\n",
    "    adata = adata[:, adata.var_names.isin(hvgs)]\n",
    "    # adata.X = adata.layers['X_norm_pearson']\n",
    "    controls3 = ['Dabrafenib', 'Belinostat', 'Dimethyl Sulfoxide']\n",
    "    # compound-cell type based split\n",
    "    sm_names = adata.obs.sm_name.unique()\n",
    "    non_controls = np.setdiff1d(sm_names, controls3)\n",
    "    test_sm_names_1 = np.random.choice(non_controls, 30)\n",
    "    test_cell_types = ['B cells', 'Myeloid cells']\n",
    "    test_mask_1 =  adata.obs.sm_name.isin(test_sm_names_1) & adata.obs.cell_type.isin(test_cell_types) # cell type compound \n",
    "    print(test_mask_1.sum())\n",
    "\n",
    "    # donor based split \n",
    "    test_donor = np.random.choice(adata.obs.donor_id.unique(), )\n",
    "    test_sm_names_2 = list(np.random.choice(non_controls, int(len(non_controls)/2)))  # half of non control compounds + controls\n",
    "    test_mask_2 =  adata.obs.donor_id.eq(test_donor) & adata.obs.sm_name.isin(test_sm_names_2) # donor\n",
    "    print(test_mask_2.sum())\n",
    "\n",
    "    # actual split \n",
    "    features_X = ['sm_name', 'cell_type', 'donor_id']\n",
    "    adata.obs['split']  = 'train'\n",
    "    adata.obs.loc[test_mask_1 | test_mask_2, 'split'] = 'test'\n",
    "\n",
    "    gene_names = adata.var_names\n",
    "    n_genes = len(gene_names)\n",
    "    adata_train = adata[adata.obs.split=='train',:]\n",
    "    df_train = pd.DataFrame(adata_train.X, columns=adata.var_names, index=pd.MultiIndex.from_frame(adata_train.obs[features_X])) \n",
    "    adata_test = adata[adata.obs.split=='test',:]\n",
    "    df_test = pd.DataFrame(adata_test.X, columns=adata.var_names, index=pd.MultiIndex.from_frame(adata_test.obs[features_X])) \n",
    "else:\n",
    "    features_X = ['sm_name', 'cell_type']\n",
    "    de_train = ad.read_h5ad('../resources/neurips-2023-data/de_train.h5ad')\n",
    "    de_test = ad.read_h5ad('../resources/neurips-2023-data/de_test.h5ad')\n",
    "\n",
    "    de_train.X = de_train.layers['sign_log10_adj_pval']\n",
    "    de_test.X = de_test.layers['sign_log10_adj_pval']\n",
    "    \n",
    "    gene_names = de_train.var_names\n",
    "    n_genes = len(gene_names)\n",
    "    \n",
    "    df_train = pd.DataFrame(de_train.X, columns=de_train.var_names, index=pd.MultiIndex.from_frame(de_train.obs[features_X])) \n",
    "    df_test = pd.DataFrame(de_test.X, columns=de_test.var_names, index=pd.MultiIndex.from_frame(de_test.obs[features_X])) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process grn net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:39:20.429159Z",
     "iopub.status.busy": "2024-05-28T09:39:20.428737Z",
     "iopub.status.idle": "2024-05-28T09:39:20.898036Z",
     "shell.execute_reply": "2024-05-28T09:39:20.897375Z",
     "shell.execute_reply.started": "2024-05-28T09:39:20.429131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.9468663664012258\n"
     ]
    }
   ],
   "source": [
    "# grn_net_df = pd.read_csv(\"https://github.com/pablormier/omnipath-static/raw/main/op/collectri-26.09.2023.zip\")\n",
    "\n",
    "grn_net_df = pd.read_csv(\"../output/benchmark/grn_models/scenicplus.csv\")\n",
    "# grn_net_df =  shuffle_grn(grn_net_df)\n",
    "\n",
    "grn_net_df = grn_net_df[grn_net_df.target.isin(gene_names)].reset_index(drop=True)\n",
    "grn_net = grn_net_df.pivot(index='target', columns='source', values='weight').fillna(0)\n",
    "\n",
    "print('sparsity:', (grn_net.values==0).sum()/grn_net.size)\n",
    "net_genes = grn_net.index.unique()\n",
    "shared_genes = np.intersect1d(net_genes, gene_names)\n",
    "missing_genes = np.setdiff1d(gene_names, shared_genes)\n",
    "tfs_n = len(grn_net.columns.unique())\n",
    "\n",
    "sparsity = (grn_net.values==0).sum()/grn_net.size\n",
    "ratios = [sparsity, (1-sparsity)/2, (1-sparsity)/2]\n",
    "shape = (missing_genes.shape[0], tfs_n)\n",
    "X_random = np.random.choice([0, -1, 1], size=shape, p=ratios)\n",
    "# concat actual net with random net\n",
    "grn_net = pd.concat([grn_net, pd.DataFrame(X_random, columns=grn_net.columns, index=missing_genes)])\n",
    "\n",
    "# make the order of genes compatible with adata\n",
    "grn_net = grn_net.reindex(gene_names)\n",
    "\n",
    "grn_net = grn_net.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:39:34.362270Z",
     "iopub.status.busy": "2024-05-28T09:39:34.361601Z",
     "iopub.status.idle": "2024-05-28T09:39:34.376341Z",
     "shell.execute_reply": "2024-05-28T09:39:34.375590Z",
     "shell.execute_reply.started": "2024-05-28T09:39:34.362217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(grn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:05:20.040330Z",
     "iopub.status.busy": "2024-05-28T09:05:20.039956Z",
     "iopub.status.idle": "2024-05-28T09:06:07.770893Z",
     "shell.execute_reply": "2024-05-28T09:06:07.770104Z",
     "shell.execute_reply.started": "2024-05-28T09:05:20.040295Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fit LOO encoders: 100%|██████████| 5317/5317 [00:47<00:00, 111.42it/s]\n"
     ]
    }
   ],
   "source": [
    "class MultiOutputTargetEncoder:\n",
    "    def __init__(self):\n",
    "        self.encoders: List[category_encoders.leave_one_out.LeaveOneOutEncoder] = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def new_encoder() -> category_encoders.leave_one_out.LeaveOneOutEncoder:\n",
    "        return category_encoders.leave_one_out.LeaveOneOutEncoder(return_df=False)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.encoders = []\n",
    "        for j in tqdm.tqdm(range(y.shape[1]), desc='fit LOO encoders'):\n",
    "            self.encoders.append(MultiOutputTargetEncoder.new_encoder())\n",
    "            self.encoders[-1].fit(X, y[:, j])\n",
    "    \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        Z = []\n",
    "        for encoder in tqdm.tqdm(self.encoders, desc='transform LOO encoders'):\n",
    "            y_hat = encoder.transform(X)\n",
    "            Z.append(y_hat)\n",
    "        Z = np.asarray(Z)\n",
    "        return np.transpose(Z, (1, 0, 2))\n",
    "encoder = MultiOutputTargetEncoder()\n",
    "\n",
    "encoder.fit(np.asarray([df_train.index.get_level_values(var) for var in features_X]).T, df_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:06:07.773598Z",
     "iopub.status.busy": "2024-05-28T09:06:07.773172Z",
     "iopub.status.idle": "2024-05-28T09:06:49.152823Z",
     "shell.execute_reply": "2024-05-28T09:06:49.151651Z",
     "shell.execute_reply.started": "2024-05-28T09:06:07.773570Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transform LOO encoders: 100%|██████████| 5317/5317 [00:20<00:00, 253.32it/s]\n",
      "transform LOO encoders: 100%|██████████| 5317/5317 [00:20<00:00, 261.40it/s]\n"
     ]
    }
   ],
   "source": [
    "X = encoder.transform(np.asarray([df_train.index.get_level_values(var) for var in features_X]).T)\n",
    "X_submit = encoder.transform(np.asarray([df_test.index.get_level_values(var) for var in features_X]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:06:49.154727Z",
     "iopub.status.busy": "2024-05-28T09:06:49.154244Z",
     "iopub.status.idle": "2024-05-28T09:06:49.160630Z",
     "shell.execute_reply": "2024-05-28T09:06:49.159739Z",
     "shell.execute_reply.started": "2024-05-28T09:06:49.154691Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402, 5317, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:06:49.161683Z",
     "iopub.status.busy": "2024-05-28T09:06:49.161458Z",
     "iopub.status.idle": "2024-05-28T09:06:49.193465Z",
     "shell.execute_reply": "2024-05-28T09:06:49.192814Z",
     "shell.execute_reply.started": "2024-05-28T09:06:49.161651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features \n",
    "        self.labels = labels \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "dataset = Dataset(features=torch.tensor(X, dtype=torch.float32), labels=torch.tensor(df_train.values, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:06:49.194482Z",
     "iopub.status.busy": "2024-05-28T09:06:49.194269Z",
     "iopub.status.idle": "2024-05-28T09:06:49.198472Z",
     "shell.execute_reply": "2024-05-28T09:06:49.197416Z",
     "shell.execute_reply.started": "2024-05-28T09:06:49.194461Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def background_noise(\n",
    "    *size: int,\n",
    "    cutoff: float = 0.05,\n",
    "    device: str = 'cuda',\n",
    "    generator: torch.Generator = None) -> torch.Tensor:\n",
    "    sign = 2 * torch.randint(0, 2, size, device=device) - 1\n",
    "\n",
    "    return sign * torch.log10(cutoff +  torch.rand(*size, generator=generator, device=device) * (1. - cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:06:49.200191Z",
     "iopub.status.busy": "2024-05-28T09:06:49.199534Z",
     "iopub.status.idle": "2024-05-28T09:06:49.205897Z",
     "shell.execute_reply": "2024-05-28T09:06:49.205199Z",
     "shell.execute_reply.started": "2024-05-28T09:06:49.200161Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5317, 942)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grn_net.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:33:58.199521Z",
     "iopub.status.busy": "2024-05-28T09:33:58.199118Z",
     "iopub.status.idle": "2024-05-28T09:33:58.214216Z",
     "shell.execute_reply": "2024-05-28T09:33:58.213389Z",
     "shell.execute_reply.started": "2024-05-28T09:33:58.199491Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "antoine_model = False\n",
    "\n",
    "class Scaler(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, m: int) -> None:\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.m: int = m\n",
    "        self.a: torch.Tensor = torch.nn.Parameter(torch.ones((1, self.m)))\n",
    "        self.b: torch.Tensor = torch.nn.Parameter(torch.zeros((1, self.m)))\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.a * X + self.b\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, n_genes:int, grn_net:np.ndarray, n_nodes_hidden:int=120):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.n_genes = n_genes\n",
    "        self.encode = len(features_X)\n",
    "        self.A = torch.tensor(grn_net, dtype=torch.float32, device='cuda', requires_grad=False)\n",
    "        # self.A = torch.eye(self.n_genes).to(device='cuda')\n",
    "        # self.A = torch.randn(n_genes, 1000).to(device='cuda')\n",
    "        self.n_tfs = self.A.shape[1]\n",
    "        \n",
    "        dropout_rate = .05\n",
    "        \n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(n_genes * self.encode, n_nodes_hidden),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Dropout(dropout_rate),  \n",
    "            \n",
    "#             nn.Linear(n_nodes_hidden, n_genes),\n",
    "#         )\n",
    "\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(n_genes, n_nodes_hidden),  \n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Dropout(dropout_rate), \n",
    "            \n",
    "#             nn.Linear(n_nodes_hidden, n_genes)\n",
    "#         )\n",
    "        # self.mlp = nn.Sequential(\n",
    "        #     nn.Linear(n_genes * self.encode, n_nodes_hidden),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.Dropout(dropout_rate),  \n",
    "        #     nn.Linear(n_nodes_hidden, 64),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.Dropout(dropout_rate),  \n",
    "        #     nn.Linear(64, 120),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.Dropout(dropout_rate),  \n",
    "        #     nn.Linear(n_nodes_hidden, n_genes)\n",
    "        # )\n",
    "        \n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(n_genes * self.encode, n_nodes_hidden),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(n_nodes_hidden, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(16, 120),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(n_nodes_hidden, n_genes * self.encode)\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(2*self.encode+self.A.shape[1], 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "        # self.mlp = nn.Sequential(\n",
    "        #     nn.Linear(n_genes * self.encode, n_nodes_hidden),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.Linear(n_nodes_hidden, 32),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.Linear(32, 120),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.Linear(n_nodes_hidden, n_genes)\n",
    "        # )\n",
    "        # self.scaler_1 = nn.Sequential(\n",
    "        #     Scaler(self.n_tfs), \n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     Scaler(self.n_tfs)\n",
    "        # )\n",
    "        # self.scaler_2 = nn.Sequential(\n",
    "        #     Scaler(n_genes), \n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     Scaler(n_genes)\n",
    "        # )\n",
    "    def reparametrize(self, mu, log_var):\n",
    "        std = torch.sqrt(torch.exp(log_var))\n",
    "        eps = torch.randn_like(log_var)\n",
    "        return mu + std*eps\n",
    "        # return mu + log_var*eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if False:\n",
    "            x = self.encoder(x.reshape(x.shape[0], -1))\n",
    "            mu, log_var = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "            mu = torch.matmul(mu, self.A)\n",
    "            log_var = torch.matmul(log_var, self.A)\n",
    "\n",
    "            x = torch.matmul(x, self.A)\n",
    "\n",
    "            z = self.reparametrize(mu, log_var)\n",
    "            x = self.decoder(x)\n",
    "        if False:\n",
    "            x = self.mlp(x.reshape(x.shape[0], -1))\n",
    "        if True:\n",
    "            x_org = x.clone()\n",
    "            dim_1, dim_2, dim_3 = x.shape\n",
    "            x = self.mlp1(x.reshape(x.shape[0], -1))\n",
    "            x = x.reshape(dim_1, dim_2, dim_3)\n",
    "\n",
    "            x = torch.cat((x, x_org), dim=2)\n",
    "            \n",
    "            # Add a new dimension at the start (dimension 0) to make it [1, 5000, 79]\n",
    "            A_rep = self.A.unsqueeze(0)\n",
    "\n",
    "            # Repeat the tensor along the new dimension to make it [20, 5000, 79]\n",
    "            A_rep = A_rep.repeat(x.shape[0], 1, 1)\n",
    "            \n",
    "            x = torch.cat((x, A_rep), dim=2) # add gene-tf weights to the encode\n",
    "            \n",
    "            x = x.reshape(-1, 2*dim_3+self.A.shape[1])\n",
    "            \n",
    "            x = self.mlp2(x)\n",
    "            \n",
    "            x = x.reshape(dim_1, dim_2)\n",
    "        if False:\n",
    "            x = self.mlp(x.reshape(x.shape[0], -1))\n",
    "            x = torch.matmul(x, self.A)\n",
    "            x = self.scaler_1(x)\n",
    "            A_inv = torch.linalg.pinv(self.A)\n",
    "            x = torch.matmul(x, A_inv)\n",
    "            x = self.scaler_2(x)\n",
    "        #return x, mu, log_var\n",
    "        return x, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:11:48.021458Z",
     "iopub.status.busy": "2024-05-28T09:11:48.020813Z",
     "iopub.status.idle": "2024-05-28T09:11:48.025985Z",
     "shell.execute_reply": "2024-05-28T09:11:48.025229Z",
     "shell.execute_reply.started": "2024-05-28T09:11:48.021424Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5317, 76])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:50:52.060917Z",
     "iopub.status.busy": "2024-05-28T09:50:52.060400Z",
     "iopub.status.idle": "2024-05-28T09:55:07.630043Z",
     "shell.execute_reply": "2024-05-28T09:55:07.628948Z",
     "shell.execute_reply.started": "2024-05-28T09:50:52.060890Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.163, R2:0.930: 100%|██████████| 100/100 [00:25<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.20594417146551347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.315, R2:0.911: 100%|██████████| 100/100 [00:25<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.06666554083018042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.249, R2:0.929: 100%|██████████| 100/100 [00:25<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.08273153517792112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.177, R2:0.938: 100%|██████████| 100/100 [00:25<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.22634402500679096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.139, R2:0.951: 100%|██████████| 100/100 [00:25<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.19364236123920395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.187, R2:0.954: 100%|██████████| 100/100 [00:25<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.2472775487196935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.354, R2:0.851: 100%|██████████| 100/100 [00:25<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.2251267031293495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.114, R2:0.955: 100%|██████████| 100/100 [00:25<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.23930045531338306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.161, R2:0.893: 100%|██████████| 100/100 [00:25<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.2587423359229923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 0.181, R2:0.943: 100%|██████████| 100/100 [00:25<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.26904458454488905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = int(df_train.shape[0]/20)\n",
    "n_epoch = 100\n",
    "\n",
    "\n",
    "# models = []\n",
    "y_pred_submit_all = []\n",
    "for i in range(10):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    model = NN(n_genes, grn_net=grn_net)\n",
    "    model = model.to('cuda')\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, eps=1e-8)\n",
    "    loss_func = lambda y_pred, y_true: torch.sum(torch.square(y_pred-y_true))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, \n",
    "                patience=5, threshold_mode='rel', threshold=0.0001, cooldown=5, min_lr=1e-10, eps=1e-8)\n",
    "\n",
    "    pbar = tqdm.tqdm(range(n_epoch))\n",
    "    for i_epoch in pbar:\n",
    "        rel_loss_store = []\n",
    "        Y_pred_stack = []\n",
    "        Y_true_stack = []\n",
    "        for batch_idx, (data_batch, label_batch) in enumerate(dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            data_batch = data_batch.to('cuda')\n",
    "            label_batch = label_batch.to('cuda')\n",
    "            \n",
    "            if antoine_model:\n",
    "                generator = torch.Generator(device='cuda').manual_seed(32)\n",
    "                label_batch = label_batch + 0.2 * background_noise(*label_batch.size(), generator=generator) #TODO: optimize the weight\n",
    "\n",
    "            x_pred, mu, log_var = model(data_batch) #forward\n",
    "\n",
    "            Y_true_stack.append(label_batch.cpu().detach().numpy())\n",
    "            Y_pred_stack.append(x_pred.cpu().detach().numpy())\n",
    "\n",
    "            loss_x = loss_func(x_pred, label_batch)\n",
    "\n",
    "            #loss_KL =  - 0.5 * torch.sum(1.0 + log_var - mu.pow(2) - log_var.exp()) #TODO: fix this\n",
    "\n",
    "            beta = 1\n",
    "            #loss = loss_x + beta*loss_KL\n",
    "            loss = loss_x \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # baseline pred\n",
    "            Y_pred_mean = torch.mean(label_batch, axis=0)\n",
    "            loss_baseline = loss_func(label_batch, Y_pred_mean)\n",
    "            rel_loss = loss_x/loss_baseline\n",
    "            rel_loss_store.append(rel_loss.item())\n",
    "        # if i_epoch%10==0:\n",
    "        #     # AUROC\n",
    "        #     mask = ~np.eye(n_genes, dtype=bool)\n",
    "        #     grn_pred = np.abs(model.A.cpu().data.numpy())\n",
    "        #     print('AUROC', roc_auc_score(np.abs(grn_net[mask]), grn_pred[mask]))\n",
    "\n",
    "        mean_rel_loss = np.mean(rel_loss_store)\n",
    "        scheduler.step(mean_rel_loss)\n",
    "\n",
    "        y_pred = np.concatenate(Y_pred_stack, axis=0)\n",
    "        y_true = np.concatenate(Y_true_stack, axis=0)\n",
    "\n",
    "        r2 = r2_score(y_true, y_pred, multioutput='variance_weighted')\n",
    "\n",
    "        pbar.set_description(f'Rel loss: {mean_rel_loss:.3f}, R2:{r2:.3f}')\n",
    "    # predict\n",
    "    model.eval()\n",
    "    y_pred_submit, mu, log_var = model(torch.tensor(X_submit, dtype=torch.float32, device='cuda'))\n",
    "    y_pred_submit = y_pred_submit.cpu().detach().numpy()\n",
    "    y_pred_submit_all.append(y_pred_submit)\n",
    "\n",
    "    print('r2:', r2_score(df_test, y_pred_submit, multioutput='variance_weighted'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-28T09:07:45.561980Z",
     "iopub.status.idle": "2024-05-28T09:07:45.562383Z",
     "shell.execute_reply": "2024-05-28T09:07:45.562237Z",
     "shell.execute_reply.started": "2024-05-28T09:07:45.562220Z"
    }
   },
   "source": [
    "- baseline:\n",
    "r2: 0.28898735855873964\n",
    "mse: 1.6771364452118458\n",
    "mrrmse: 0.7764429858370391\n",
    "\n",
    "-scenicplus: \n",
    "r2: 0.3008508432154561\n",
    "mse: 1.6491528604971506\n",
    "mrrmse: 0.7628936673821836\n",
    "\n",
    "-scenicplus (shuffled): \n",
    "r2: 0.2857228691302343\n",
    "mse: 1.6848367220795772\n",
    "mrrmse: 0.7788572802129669\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T09:50:04.616579Z",
     "iopub.status.busy": "2024-05-28T09:50:04.616152Z",
     "iopub.status.idle": "2024-05-28T09:50:04.667918Z",
     "shell.execute_reply": "2024-05-28T09:50:04.667110Z",
     "shell.execute_reply.started": "2024-05-28T09:50:04.616548Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of models: 10\n",
      "r2: 0.29235347369238274\n",
      "mse: 1.669196453655622\n",
      "mrrmse: 0.7747873985104607\n"
     ]
    }
   ],
   "source": [
    "print('number of models:', len(y_pred_submit_all))\n",
    "y_pred_submit_mean = np.mean(y_pred_submit_all, axis=0)\n",
    "print('r2:', r2_score(df_test, y_pred_submit_mean, multioutput='variance_weighted'))\n",
    "print('mse:', mean_squared_error(y_pred_submit_mean, df_test.values))\n",
    "print('mrrmse:', mrrmse(y_pred_submit_mean, df_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-28T09:07:45.566012Z",
     "iopub.status.idle": "2024-05-28T09:07:45.566268Z",
     "shell.execute_reply": "2024-05-28T09:07:45.566155Z",
     "shell.execute_reply.started": "2024-05-28T09:07:45.566143Z"
    }
   },
   "outputs": [],
   "source": [
    "r2: 0.24227989212421114\n",
    "mse: 1.7873100056454099\n",
    "mrrmse: 0.8062594076816958"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-28T09:07:45.567217Z",
     "iopub.status.idle": "2024-05-28T09:07:45.567517Z",
     "shell.execute_reply": "2024-05-28T09:07:45.567378Z",
     "shell.execute_reply.started": "2024-05-28T09:07:45.567364Z"
    }
   },
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-28T09:07:45.568596Z",
     "iopub.status.idle": "2024-05-28T09:07:45.569002Z",
     "shell.execute_reply": "2024-05-28T09:07:45.568800Z",
     "shell.execute_reply.started": "2024-05-28T09:07:45.568781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD, KernelPCA    \n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "\n",
    "\n",
    "def encode(df_train, df_test, feature_space):\n",
    "    # encode each factor\n",
    "    x_encoded_dict = {}\n",
    "    for feature in feature_space:\n",
    "        index = df_train.index.get_level_values(feature)\n",
    "        n_com = min([len(index.unique()), 50])\n",
    "        # var_x = TruncatedSVD(n_components=n_com, n_iter=12, random_state=random_state).fit_transform(df_train)\n",
    "        var_x = KernelPCA(n_components=n_com, kernel='linear', random_state=random_state).fit_transform(df_train)\n",
    "        x_encoded = pd.DataFrame(var_x, index=index).reset_index()\n",
    "        x_encoded = x_encoded.groupby(feature).mean()\n",
    "        x_encoded_dict[feature] = x_encoded\n",
    "    # create X and X_submit\n",
    "    X = []\n",
    "    X_submit = []\n",
    "    for i_feature, feature in enumerate(feature_space):\n",
    "        # encode train data\n",
    "        index = df_train.index.get_level_values(feature)\n",
    "        feature_encoded = np.asarray([x_encoded_dict[feature].loc[name].values for name in index])\n",
    "        if i_feature == 0:\n",
    "            X = feature_encoded\n",
    "        else: \n",
    "            X = np.concatenate([X, feature_encoded], axis=1)\n",
    "        \n",
    "        # encode test data\n",
    "        index = df_test.index.get_level_values(feature)\n",
    "        feature_encoded = np.asarray([x_encoded_dict[feature].loc[name].values for name in index])\n",
    "        if i_feature == 0:\n",
    "            X_submit = feature_encoded\n",
    "        else: \n",
    "            X_submit = np.concatenate([X_submit, feature_encoded], axis=1)\n",
    "    return X, X_submit\n",
    "\n",
    "\n",
    "random_state = 32\n",
    "n_components = 50\n",
    "X_rf, X_submit_rf = encode(df_train, df_test, features_X)\n",
    "emb_model = RandomForestRegressor(n_estimators=100, random_state=random_state)\n",
    "reducer = TruncatedSVD(n_components=n_components, n_iter=12, random_state=random_state)\n",
    "Y = reducer.fit_transform(df_train)\n",
    "\n",
    "emb_model.fit(X_rf, Y)\n",
    "y_pred_submit = reducer.inverse_transform(emb_model.predict(X_submit_rf))\n",
    "\n",
    "print('r2:', r2_score(df_test, y_pred_submit, multioutput='variance_weighted'))\n",
    "print('mse:', mean_squared_error(y_pred_submit, df_test.values))\n",
    "print('mrrmse:', mrrmse(y_pred_submit, df_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-28T09:07:45.570054Z",
     "iopub.status.idle": "2024-05-28T09:07:45.570333Z",
     "shell.execute_reply": "2024-05-28T09:07:45.570215Z",
     "shell.execute_reply.started": "2024-05-28T09:07:45.570202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2: 0.24227989212421114\n",
    "mse: 1.7873100056454099\n",
    "mrrmse: 0.8062594076816958"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF activity encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_293774/1701922426.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  x_encoded = x_encoded.groupby(feature).mean()\n",
      "/tmp/ipykernel_293774/1701922426.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  x_encoded = x_encoded.groupby(feature).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.29187043844764465\n",
      "mse: 1.6703358370730952\n",
      "mrrmse: 0.7474491838704398\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD, KernelPCA    \n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "\n",
    "\n",
    "def encode(df_train, df_test, feature_space):\n",
    "    # encode each factor\n",
    "    x_encoded_dict = {}\n",
    "    for feature in feature_space:\n",
    "        index = df_train.index.get_level_values(feature)\n",
    "        n_com = min([len(index.unique()), 50])\n",
    "        # var_x = TruncatedSVD(n_components=n_com, n_iter=12, random_state=random_state).fit_transform(df_train)\n",
    "        var_x = KernelPCA(n_components=n_com, kernel='linear', random_state=random_state).fit_transform(df_train)\n",
    "        x_encoded = pd.DataFrame(var_x, index=index).reset_index()\n",
    "        x_encoded = x_encoded.groupby(feature).mean()\n",
    "        x_encoded_dict[feature] = x_encoded\n",
    "    # create X and X_submit\n",
    "    X = []\n",
    "    X_submit = []\n",
    "    for i_feature, feature in enumerate(feature_space):\n",
    "        # encode train data\n",
    "        index = df_train.index.get_level_values(feature)\n",
    "        feature_encoded = np.asarray([x_encoded_dict[feature].loc[name].values for name in index])\n",
    "        if i_feature == 0:\n",
    "            X = feature_encoded\n",
    "        else: \n",
    "            X = np.concatenate([X, feature_encoded], axis=1)\n",
    "        \n",
    "        # encode test data\n",
    "        index = df_test.index.get_level_values(feature)\n",
    "        feature_encoded = np.asarray([x_encoded_dict[feature].loc[name].values for name in index])\n",
    "        if i_feature == 0:\n",
    "            X_submit = feature_encoded\n",
    "        else: \n",
    "            X_submit = np.concatenate([X_submit, feature_encoded], axis=1)\n",
    "    return X, X_submit\n",
    "\n",
    "\n",
    "random_state = 32\n",
    "n_components = 50\n",
    "\n",
    "grn_net_shuffled = grn_net.copy()\n",
    "np.random.shuffle(grn_net_shuffled)\n",
    "tf_activities = df_train.values.dot(grn_net_shuffled)\n",
    "tf_activities = pd.DataFrame(tf_activities, index=df_train.index)\n",
    "\n",
    "X_rf, X_submit_rf = encode(tf_activities, df_test, features_X)\n",
    "\n",
    "emb_model = RandomForestRegressor(n_estimators=100, random_state=random_state)\n",
    "reducer = TruncatedSVD(n_components=n_components, n_iter=12, random_state=random_state)\n",
    "Y = reducer.fit_transform(df_train)\n",
    "\n",
    "emb_model.fit(X_rf, Y)\n",
    "y_pred_submit = reducer.inverse_transform(emb_model.predict(X_submit_rf))\n",
    "\n",
    "print('r2:', r2_score(df_test, y_pred_submit, multioutput='variance_weighted'))\n",
    "print('mse:', mean_squared_error(y_pred_submit, df_test.values))\n",
    "print('mrrmse:', mrrmse(y_pred_submit, df_test.values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
