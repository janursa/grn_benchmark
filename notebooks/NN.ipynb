{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb960c30950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 16\n",
    "use_gpu = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad('..//output/preprocess/bulk_adata_f.h5ad')\n",
    "df = pd.DataFrame(adata.X, index=pd.MultiIndex.from_frame(adata.obs[['cell_type', 'sm_name', 'plate_name']]), columns=adata.var_names)\n",
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rel loss: 1.524, R2:-0.978:   4%|‚ñç         | 3/80 [00:05<02:27,  1.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRel loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_rel_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R2:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m--> 108\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 88\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_data, batch_size, n_epoch)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# calculate loss and backpropagate \u001b[39;00m\n\u001b[1;32m     87\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(Y_true, Y_pred)\n\u001b[0;32m---> 88\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# baseline pred\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py10/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, n_genes:int, n_nodes_latent:int=16):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.n_genes = n_genes\n",
    "        dropout_p = 0.2\n",
    "\n",
    "        bias = False\n",
    "        nonlinear = torch.nn.PReLU()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_genes, 64),\n",
    "            nonlinear,\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "\n",
    "            torch.nn.Linear(64, n_nodes_latent*2, bias=bias),\n",
    "            nonlinear\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_nodes_latent, n_genes, bias=bias)\n",
    "        )\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(module.weight, nonlinearity='relu', mode='fan_in')\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.constant_(module.bias, 0.001)\n",
    "\n",
    "    def reparametrize(self, mu, log_var):\n",
    "        # max_log_var = 100\n",
    "        # log_var = torch.clamp(log_var, max=-max_log_var)\n",
    "        # print(log_var)\n",
    "        # aaa\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        # print(std)\n",
    "        # return mu + std*eps\n",
    "        return mu + log_var*eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(x,2, dim=1)\n",
    "        x = self.reparametrize(mu, log_var)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "#TODO: dataloader = DataLoader(train_data, batch_size=self.opt.batch_size, shuffle=True, num_workers=1)\n",
    "#TODO: how to account for batch effects        \n",
    "def train(train_data: np.array, batch_size = 400, n_epoch = 40):\n",
    "    # train_data = (train_data - train_data.mean(axis=0))/(train_data.std(axis=0) +1e-8) #TODO: take care of this norm\n",
    "    # train_data = np.log10(train_data)\n",
    "    # print(train_data)\n",
    "    # aa\n",
    "\n",
    "    # torch data\n",
    "    train_data = torch.FloatTensor(train_data)\n",
    "    if use_gpu:\n",
    "        train_data = train_data.cuda()\n",
    "    \n",
    "    model = NN(n_genes=train_data.shape[1])\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "    #TODO: loss function. cross entropy loss? other types  \n",
    "    criterion = lambda Y_true, Y_pred: torch.mean(torch.abs(Y_true-Y_pred))\n",
    "    # optimizer\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=.00001, momentum=.9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, eps=1e-7)\n",
    "    \n",
    "    # scheduler = torch.lr_scheduler.StepLR(optimizer, step_size=10, gamma=.1) \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, \n",
    "                patience=5, threshold_mode='rel', threshold=0.0001, cooldown=5, min_lr=1e-10, eps=1e-8)\n",
    "\n",
    "    pbar = tqdm.tqdm(range(n_epoch))\n",
    "    for i_epoch in pbar:\n",
    "        # shuffle the data\n",
    "        train_data = train_data[torch.randperm(train_data.size(dim=0))]\n",
    "        # train for a epoch\n",
    "        model.train()\n",
    "        rel_loss_store = []\n",
    "        Y_pred_stack = []\n",
    "        Y_true_stack = []\n",
    "        for X in np.array_split(train_data, batch_size, axis=0):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Y_true = X #TODO: add noise to the data\n",
    "            Y_pred = model.forward(X)\n",
    "            \n",
    "            Y_true_stack.append(Y_true.cpu().detach().numpy())\n",
    "            Y_pred_stack.append(Y_pred.cpu().detach().numpy())\n",
    "            # calculate loss and backpropagate \n",
    "            loss = criterion(Y_true, Y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # baseline pred\n",
    "            Y_pred_mean = torch.mean(Y_true, axis=0)\n",
    "            loss_baseline = criterion(Y_true, Y_pred_mean)\n",
    "            rel_loss = loss/loss_baseline\n",
    "            rel_loss_store.append(rel_loss.item())\n",
    "\n",
    "        mean_rel_loss = np.mean(rel_loss_store)\n",
    "        scheduler.step(mean_rel_loss)\n",
    "\n",
    "        y_pred = np.concatenate(Y_pred_stack, axis=0)\n",
    "        y_true = np.concatenate(Y_true_stack, axis=0)\n",
    "\n",
    "        r2 = r2_score(y_true, y_pred, multioutput='variance_weighted')\n",
    "\n",
    "        pbar.set_description(f'Rel loss: {mean_rel_loss:.3f}, R2:{r2:.3f}')\n",
    "    return model\n",
    "            \n",
    "model = train(data, batch_size = 400, n_epoch = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = 10\n",
    "np.log10(10**2) -> 2*log10 -> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
